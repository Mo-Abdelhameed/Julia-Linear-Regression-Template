{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using DataFrames\n",
    "using LazyJSON\n",
    "using GLM \n",
    "using MLJ \n",
    "using MLJBase\n",
    "using CSV\n",
    "using Serialization\n",
    "using MLJScientificTypes\n",
    "using CategoricalArrays\n",
    "using JLD2\n",
    "using StatsBase\n",
    "using StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up directories\n",
    "ROOT_DIR = dirname(pwd())\n",
    "# Setting up directory and file paths\n",
    "MODEL_INPUTS_OUTPUTS = joinpath(ROOT_DIR, \"model_inputs_outputs\")\n",
    "INPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = joinpath(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = joinpath(INPUT_DIR, \"data\")\n",
    "TRAIN_DIR = joinpath(DATA_DIR, \"training\")\n",
    "TEST_DIR = joinpath(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = joinpath(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = joinpath(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"ohe.jld2\")\n",
    "PREDICTOR_DIR_PATH = joinpath(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = joinpath(PREDICTOR_DIR_PATH, \"predictor.jld2\")\n",
    "IMPUTATION_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"imputation.ser\")\n",
    "TOP_CATEGORIES = joinpath(MODEL_ARTIFACTS_PATH, \"top_categories.ser\")\n",
    "\n",
    "if !isdir(MODEL_ARTIFACTS_PATH)\n",
    "    mkdir(MODEL_ARTIFACTS_PATH)\n",
    "end\n",
    "if !isdir(PREDICTOR_DIR_PATH)\n",
    "    mkdir(PREDICTOR_DIR_PATH)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"percent_pell_grant\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading a schema from a JSON file and extracting features\n",
    "file_name = first(filter(x -> endswith(x, \"json\"), readdir(INPUT_SCHEMA_DIR)))\n",
    "schema_path = joinpath(INPUT_SCHEMA_DIR, file_name)\n",
    "schema_string = read(schema_path, String)  # Read file content as a string\n",
    "schema = LazyJSON.parse(schema_string)  # Parse using LazyJSON\n",
    "\n",
    "features = schema[\"features\"]\n",
    "\n",
    "# Identifying numeric, categorical, and nullable features\n",
    "numeric_features = String[]\n",
    "categorical_features = String[]\n",
    "nullable_features = String[]\n",
    "\n",
    "for f in features\n",
    "    if f[\"dataType\"] == \"CATEGORICAL\"\n",
    "        push!(categorical_features, f[\"name\"])\n",
    "    else\n",
    "        push!(numeric_features, f[\"name\"])\n",
    "    end\n",
    "    if f[\"nullable\"]\n",
    "        push!(nullable_features, f[\"name\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Extracting ID and target features\n",
    "id_feature = schema[\"id\"][\"name\"]\n",
    "target_feature = schema[\"target\"][\"name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5 rows × 42 columns (omitted printing of 37 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>unit_id</th><th>act_combined_midrange</th><th>act_english_midrange</th><th>act_math_midrange</th><th>act_writing_midrange</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Union{Missing, Float64}\">Float64?</th><th title=\"Union{Missing, Float64}\">Float64?</th><th title=\"Union{Missing, Float64}\">Float64?</th><th title=\"Union{Missing, Float64}\">Float64?</th></tr></thead><tbody><tr><th>1</th><td>459523</td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td></tr><tr><th>2</th><td>123341</td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td></tr><tr><th>3</th><td>172495</td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td></tr><tr><th>4</th><td>419457</td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td></tr><tr><th>5</th><td>455725</td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td><td><em>missing</em></td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& unit\\_id & act\\_combined\\_midrange & act\\_english\\_midrange & act\\_math\\_midrange & act\\_writing\\_midrange & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64? & Float64? & Float64? & Float64? & \\\\\n",
       "\t\\hline\n",
       "\t1 & 459523 & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & $\\dots$ \\\\\n",
       "\t2 & 123341 & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & $\\dots$ \\\\\n",
       "\t3 & 172495 & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & $\\dots$ \\\\\n",
       "\t4 & 419457 & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & $\\dots$ \\\\\n",
       "\t5 & 455725 & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×42 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m unit_id \u001b[0m\u001b[1m act_combined_midrange \u001b[0m\u001b[1m act_english_midrange \u001b[0m\u001b[1m act_math_midrange\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64   \u001b[0m\u001b[90m Float64?              \u001b[0m\u001b[90m Float64?             \u001b[0m\u001b[90m Float64?         \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  459523 \u001b[90m               missing \u001b[0m\u001b[90m              missing \u001b[0m\u001b[90m           missing\u001b[0m ⋯\n",
       "   2 │  123341 \u001b[90m               missing \u001b[0m\u001b[90m              missing \u001b[0m\u001b[90m           missing\u001b[0m\n",
       "   3 │  172495 \u001b[90m               missing \u001b[0m\u001b[90m              missing \u001b[0m\u001b[90m           missing\u001b[0m\n",
       "   4 │  419457 \u001b[90m               missing \u001b[0m\u001b[90m              missing \u001b[0m\u001b[90m           missing\u001b[0m\n",
       "   5 │  455725 \u001b[90m               missing \u001b[0m\u001b[90m              missing \u001b[0m\u001b[90m           missing\u001b[0m ⋯\n",
       "\u001b[36m                                                              38 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find the first CSV file in the TRAIN_DIR\n",
    "file_name = first(filter(x -> endswith(x, \".csv\"), readdir(TRAIN_DIR)))\n",
    "file_path = joinpath(TRAIN_DIR, file_name)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = CSV.File(file_path) |> DataFrame\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "first(df, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing data\n",
    "imputation_values = Dict{String, Any}()\n",
    "\n",
    "for column in nullable_features\n",
    "    if column in numeric_features\n",
    "        value = median(skipmissing(df[!, column]))\n",
    "    else\n",
    "        value = mode(skipmissing(df[!, column]))\n",
    "    end\n",
    "    df[!, column] = coalesce.(df[!, column], value)\n",
    "    imputation_values[column] = value\n",
    "end\n",
    "\n",
    "# Serialize the imputation_values dictionary to a binary file\n",
    "open(IMPUTATION_FILE, \"w\") do io\n",
    "    serialize(io, imputation_values)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5,650 rows × 40 columns (omitted printing of 36 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>act_combined_midrange</th><th>act_english_midrange</th><th>act_math_midrange</th><th>act_writing_midrange</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>2</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>3</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>4</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>5</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>6</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>7</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>8</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>9</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>10</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>11</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>12</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>13</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>14</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>15</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>16</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>17</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>18</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>19</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>20</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>21</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>22</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>23</th><td>24.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>24</th><td>23.0</td><td>22.0</td><td>22.0</td><td>7.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& act\\_combined\\_midrange & act\\_english\\_midrange & act\\_math\\_midrange & act\\_writing\\_midrange & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t2 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t3 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t4 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t5 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t6 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t7 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t8 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t9 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t10 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t11 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t12 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t13 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t14 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t15 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t16 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t17 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t18 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t19 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t20 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t21 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t22 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t23 & 24.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t24 & 23.0 & 22.0 & 22.0 & 7.0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5650×40 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m act_combined_midrange \u001b[0m\u001b[1m act_english_midrange \u001b[0m\u001b[1m act_math_midrange \u001b[0m\u001b[1m act_wr\u001b[0m ⋯\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Float64               \u001b[0m\u001b[90m Float64              \u001b[0m\u001b[90m Float64           \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "──────┼─────────────────────────────────────────────────────────────────────────\n",
       "    1 │                  23.0                  22.0               22.0         ⋯\n",
       "    2 │                  23.0                  22.0               22.0\n",
       "    3 │                  23.0                  22.0               22.0\n",
       "    4 │                  23.0                  22.0               22.0\n",
       "    5 │                  23.0                  22.0               22.0         ⋯\n",
       "    6 │                  23.0                  22.0               22.0\n",
       "    7 │                  23.0                  22.0               22.0\n",
       "    8 │                  23.0                  22.0               22.0\n",
       "  ⋮   │           ⋮                     ⋮                    ⋮                 ⋱\n",
       " 5644 │                  23.0                  22.0               22.0         ⋯\n",
       " 5645 │                  22.0                  21.0               22.0\n",
       " 5646 │                  23.0                  22.0               22.0\n",
       " 5647 │                  23.0                  22.0               22.0\n",
       " 5648 │                  23.0                  21.0               23.0         ⋯\n",
       " 5649 │                  20.0                  19.0               20.0\n",
       " 5650 │                  23.0                  22.0               22.0\n",
       "\u001b[36m                                                37 columns and 5635 rows omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving the id and target columns in a different variable\n",
    "ids = df[!, Symbol(id_feature)]\n",
    "target = df[!, Symbol(target_feature)]\n",
    "\n",
    "# Dropping the id and target from the DataFrame\n",
    "select!(df, Not([Symbol(id_feature), Symbol(target_feature)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top 3 categories\n",
    "function get_top_categories(df, features, n=3)\n",
    "    top_cats = Dict()\n",
    "    for feature in features\n",
    "        col_data = df[!, feature]\n",
    "        category_counts = countmap(col_data)\n",
    "        sorted_categories = sort(collect(category_counts), by=x->x[2], rev=true)\n",
    "        top_cats[feature] = [x[1] for x in sorted_categories[1:n]]\n",
    "    end\n",
    "    return top_cats\n",
    "end\n",
    "\n",
    "\n",
    "# Get top 3 categories for specific features\n",
    "top_categories = get_top_categories(df, categorical_features)\n",
    "\n",
    "# Function to one-hot encode only the top 3 categories\n",
    "function one_hot_top_categories!(df, top_categories)\n",
    "    for (feature, top_cats) in top_categories\n",
    "        for cat in top_cats\n",
    "            new_col_name = \"$(feature)_$(cat)\"\n",
    "            df[!, new_col_name] = df[!, feature] .== cat\n",
    "        end\n",
    "        select!(df, Not(Symbol(feature)))  # Drop the original feature column \n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "# Apply one-hot encoding\n",
    "one_hot_top_categories!(df, top_categories)\n",
    "\n",
    "\n",
    "\n",
    "# Serialize the imputation_values dictionary to a binary file\n",
    "open(TOP_CATEGORIES, \"w\") do io\n",
    "    serialize(io, top_categories)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the new DataFrame with the existing 'df' DataFrame horizontally\n",
    "df = hcat(df, DataFrame(target = target))\n",
    "\n",
    "# Get all column names\n",
    "all_columns = names(df)\n",
    "\n",
    "# Remove the target variable to get only predictor variables\n",
    "predictor_columns = filter(x -> x != \"target\", all_columns)\n",
    "\n",
    "# Create Terms for predictors and response\n",
    "predictor_terms = [Term(Symbol(col)) for col in predictor_columns]\n",
    "response_term = Term(:target)\n",
    "\n",
    "# Create the formula\n",
    "formula_obj = FormulaTerm(response_term, sum(predictor_terms))\n",
    "\n",
    "# Create a Linear Regression model and train it\n",
    "model = lm(formula_obj, df)\n",
    "\n",
    "# Your model is now trained on all predictor variables in 'df'\n",
    "\n",
    "# BEGIN\n",
    "    # If you want to perform additional operations on the model, you can do so here\n",
    "    # For example, you can set hyperparameters, do cross-validation, etc.\n",
    "    # model = ...\n",
    "# END\n",
    "\n",
    "# Saving the model to use it for predictions\n",
    "save(PREDICTOR_FILE_PATH, \"model\", model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
