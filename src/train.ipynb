{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THESE LINES\n",
    "using Suppressor\n",
    "@suppress begin\n",
    "    using Pkg\n",
    "    using DataFrames\n",
    "    using LazyJSON\n",
    "    using GLM \n",
    "    using MLJ \n",
    "    using MLJBase\n",
    "    using CSV\n",
    "    using Serialization\n",
    "    using MLJScientificTypes\n",
    "    using CategoricalArrays\n",
    "    using StatsBase\n",
    "    using StatsModels\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES\n",
    "ROOT_DIR = dirname(pwd())\n",
    "MODEL_INPUTS_OUTPUTS = joinpath(ROOT_DIR, \"model_inputs_outputs\")\n",
    "INPUT_DIR = joinpath(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = joinpath(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = joinpath(INPUT_DIR, \"data\")\n",
    "TRAIN_DIR = joinpath(DATA_DIR, \"training\")\n",
    "TEST_DIR = joinpath(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = joinpath(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = joinpath(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"ohe.jld2\")\n",
    "PREDICTOR_DIR_PATH = joinpath(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = joinpath(PREDICTOR_DIR_PATH, \"predictor.jld2\")\n",
    "IMPUTATION_FILE = joinpath(MODEL_ARTIFACTS_PATH, \"imputation.ser\")\n",
    "TOP_CATEGORIES = joinpath(MODEL_ARTIFACTS_PATH, \"top_categories.ser\")\n",
    "\n",
    "if !isdir(MODEL_ARTIFACTS_PATH)\n",
    "    mkdir(MODEL_ARTIFACTS_PATH)\n",
    "end\n",
    "if !isdir(PREDICTOR_DIR_PATH)\n",
    "    mkdir(PREDICTOR_DIR_PATH)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema\n",
    "The schema contains metadata about the datasets. We will use the scehma to get information about the type of each feature (NUMERIC or CATEGORICAL) and the id and target features, this will be helpful in preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SalePrice\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a schema from a JSON file and extracting features\n",
    "file_name = first(filter(x -> endswith(x, \"json\"), readdir(INPUT_SCHEMA_DIR)))\n",
    "schema_path = joinpath(INPUT_SCHEMA_DIR, file_name)\n",
    "schema_string = read(schema_path, String)  # Read file content as a string\n",
    "schema = LazyJSON.parse(schema_string)  # Parse using LazyJSON\n",
    "\n",
    "features = schema[\"features\"]\n",
    "\n",
    "# Identifying numeric, categorical, and nullable features\n",
    "numeric_features = String[]\n",
    "categorical_features = String[]\n",
    "nullable_features = String[]\n",
    "\n",
    "for f in features\n",
    "    if f[\"dataType\"] == \"CATEGORICAL\"\n",
    "        push!(categorical_features, f[\"name\"])\n",
    "    else\n",
    "        push!(numeric_features, f[\"name\"])\n",
    "    end\n",
    "    if f[\"nullable\"]\n",
    "        push!(nullable_features, f[\"name\"])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Extracting ID and target features\n",
    "id_feature = schema[\"id\"][\"name\"]\n",
    "target_feature = schema[\"target\"][\"name\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5 rows × 81 columns (omitted printing of 73 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>Id</th><th>MSSubClass</th><th>MSZoning</th><th>LotFrontage</th><th>LotArea</th><th>Street</th><th>Alley</th><th>LotShape</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"String7\">String7</th><th title=\"Union{Missing, Float64}\">Float64?</th><th title=\"Int64\">Int64</th><th title=\"String7\">String7</th><th title=\"Union{Missing, String7}\">String7?</th><th title=\"String3\">String3</th></tr></thead><tbody><tr><th>1</th><td>677</td><td>70</td><td>RM</td><td>60.0</td><td>9600</td><td>Pave</td><td>Grvl</td><td>Reg</td></tr><tr><th>2</th><td>62</td><td>75</td><td>RM</td><td>60.0</td><td>7200</td><td>Pave</td><td><em>missing</em></td><td>Reg</td></tr><tr><th>3</th><td>1018</td><td>120</td><td>RL</td><td><em>missing</em></td><td>5814</td><td>Pave</td><td><em>missing</em></td><td>IR1</td></tr><tr><th>4</th><td>1012</td><td>90</td><td>RL</td><td>75.0</td><td>9825</td><td>Pave</td><td><em>missing</em></td><td>Reg</td></tr><tr><th>5</th><td>570</td><td>90</td><td>RL</td><td><em>missing</em></td><td>7032</td><td>Pave</td><td><em>missing</em></td><td>IR1</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& Id & MSSubClass & MSZoning & LotFrontage & LotArea & Street & Alley & LotShape & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & String7 & Float64? & Int64 & String7 & String7? & String3 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 677 & 70 & RM & 60.0 & 9600 & Pave & Grvl & Reg & $\\dots$ \\\\\n",
       "\t2 & 62 & 75 & RM & 60.0 & 7200 & Pave & \\emph{missing} & Reg & $\\dots$ \\\\\n",
       "\t3 & 1018 & 120 & RL & \\emph{missing} & 5814 & Pave & \\emph{missing} & IR1 & $\\dots$ \\\\\n",
       "\t4 & 1012 & 90 & RL & 75.0 & 9825 & Pave & \\emph{missing} & Reg & $\\dots$ \\\\\n",
       "\t5 & 570 & 90 & RL & \\emph{missing} & 7032 & Pave & \\emph{missing} & IR1 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×81 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Id    \u001b[0m\u001b[1m MSSubClass \u001b[0m\u001b[1m MSZoning \u001b[0m\u001b[1m LotFrontage \u001b[0m\u001b[1m LotArea \u001b[0m\u001b[1m Street  \u001b[0m\u001b[1m Alley    \u001b[0m\u001b[1m L\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m Int64      \u001b[0m\u001b[90m String7  \u001b[0m\u001b[90m Float64?    \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m String7? \u001b[0m\u001b[90m S\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │   677          70  RM               60.0     9600  Pave     Grvl      R ⋯\n",
       "   2 │    62          75  RM               60.0     7200  Pave    \u001b[90m missing  \u001b[0m R\n",
       "   3 │  1018         120  RL       \u001b[90m   missing   \u001b[0m    5814  Pave    \u001b[90m missing  \u001b[0m I\n",
       "   4 │  1012          90  RL               75.0     9825  Pave    \u001b[90m missing  \u001b[0m R\n",
       "   5 │   570          90  RL       \u001b[90m   missing   \u001b[0m    7032  Pave    \u001b[90m missing  \u001b[0m I ⋯\n",
       "\u001b[36m                                                              74 columns omitted\u001b[0m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the first CSV file in the TRAIN_DIR\n",
    "file_name = first(filter(x -> endswith(x, \".csv\"), readdir(TRAIN_DIR)))\n",
    "file_path = joinpath(TRAIN_DIR, file_name)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = CSV.File(file_path) |> DataFrame\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "first(df, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing is very important before training the model, as the data may contain missing values in some cells. Moreover, most of the learning algorithms cannot work with categorical data, thus the data has to be encoded.\n",
    "\n",
    "In this section we will impute the missing values and encode the categorical features. Afterwards the data will be ready to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing missing data\n",
    "> The median value will be used to impute missing values of the numeric features and the mode will be used to impute categorical features.\n",
    "\n",
    "##### You can add your own preprocessing steps such as:\n",
    "<ul>\n",
    "<li>Normalization</li> <br>\n",
    "<li>Outlier removal</li><br>\n",
    "<li>Dropping or adding features</li><br>\n",
    "</ul>\n",
    "\n",
    "### Important note:\n",
    "<p> \n",
    "Saving the values used for imputation during training step is crucial. These values will be used to impute missing data in the testing set. This is very important to avoid the well known problem of data leakage. During testing, you should not make any assumptions about the data in hand, alternatively anything needed during the testing phase should be learned from the training phase. This is why we are creating a dictionary of values used during training to reuse these values during testing.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing data\n",
    "imputation_values = Dict{String, Any}()\n",
    "\n",
    "for column in nullable_features\n",
    "    if column in numeric_features\n",
    "        value = median(skipmissing(df[!, column]))\n",
    "    else\n",
    "        value = mode(skipmissing(df[!, column]))\n",
    "    end\n",
    "    df[!, column] = coalesce.(df[!, column], value)\n",
    "    imputation_values[column] = value\n",
    "end\n",
    "\n",
    "# Serialize the imputation_values dictionary to a binary file\n",
    "open(IMPUTATION_FILE, \"w\") do io\n",
    "    serialize(io, imputation_values)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>1,168 rows × 79 columns (omitted printing of 71 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>MSSubClass</th><th>MSZoning</th><th>LotFrontage</th><th>LotArea</th><th>Street</th><th>Alley</th><th>LotShape</th><th>LandContour</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"String7\">String7</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"String7\">String7</th><th title=\"String7\">String7</th><th title=\"String3\">String3</th><th title=\"String3\">String3</th></tr></thead><tbody><tr><th>1</th><td>70</td><td>RM</td><td>60.0</td><td>9600</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>2</th><td>75</td><td>RM</td><td>60.0</td><td>7200</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>3</th><td>120</td><td>RL</td><td>69.5</td><td>5814</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>4</th><td>90</td><td>RL</td><td>75.0</td><td>9825</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>5</th><td>90</td><td>RL</td><td>69.5</td><td>7032</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>6</th><td>20</td><td>RL</td><td>69.5</td><td>11120</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>7</th><td>60</td><td>RL</td><td>76.0</td><td>9120</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>8</th><td>50</td><td>RL</td><td>66.0</td><td>21780</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>9</th><td>60</td><td>RL</td><td>63.0</td><td>8199</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>10</th><td>90</td><td>RL</td><td>69.5</td><td>11500</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>11</th><td>20</td><td>RL</td><td>95.0</td><td>13651</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>12</th><td>60</td><td>FV</td><td>65.0</td><td>8125</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>13</th><td>20</td><td>RL</td><td>96.0</td><td>12456</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>14</th><td>20</td><td>RL</td><td>90.0</td><td>14115</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>15</th><td>20</td><td>RL</td><td>75.0</td><td>10125</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>16</th><td>120</td><td>RL</td><td>55.0</td><td>7892</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>17</th><td>60</td><td>RL</td><td>70.0</td><td>8400</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>18</th><td>20</td><td>RL</td><td>65.0</td><td>8767</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>19</th><td>80</td><td>RL</td><td>65.0</td><td>10482</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>20</th><td>60</td><td>RL</td><td>92.0</td><td>9920</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>21</th><td>50</td><td>RL</td><td>55.0</td><td>7642</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>22</th><td>20</td><td>RL</td><td>69.5</td><td>19900</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>23</th><td>90</td><td>RL</td><td>92.0</td><td>12108</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>24</th><td>20</td><td>RL</td><td>78.0</td><td>10264</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>25</th><td>20</td><td>RL</td><td>85.0</td><td>11049</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>26</th><td>190</td><td>RL</td><td>107.0</td><td>10615</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Bnk</td></tr><tr><th>27</th><td>50</td><td>RL</td><td>64.0</td><td>13053</td><td>Pave</td><td>Pave</td><td>Reg</td><td>Bnk</td></tr><tr><th>28</th><td>20</td><td>RL</td><td>70.0</td><td>7931</td><td>Pave</td><td>Grvl</td><td>Reg</td><td>Lvl</td></tr><tr><th>29</th><td>20</td><td>RL</td><td>93.0</td><td>15306</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>HLS</td></tr><tr><th>30</th><td>20</td><td>RL</td><td>77.0</td><td>9320</td><td>Pave</td><td>Grvl</td><td>IR1</td><td>Lvl</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& MSSubClass & MSZoning & LotFrontage & LotArea & Street & Alley & LotShape & LandContour & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String7 & Float64 & Int64 & String7 & String7 & String3 & String3 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 70 & RM & 60.0 & 9600 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t2 & 75 & RM & 60.0 & 7200 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t3 & 120 & RL & 69.5 & 5814 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t4 & 90 & RL & 75.0 & 9825 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t5 & 90 & RL & 69.5 & 7032 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t6 & 20 & RL & 69.5 & 11120 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t7 & 60 & RL & 76.0 & 9120 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t8 & 50 & RL & 66.0 & 21780 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t9 & 60 & RL & 63.0 & 8199 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t10 & 90 & RL & 69.5 & 11500 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t11 & 20 & RL & 95.0 & 13651 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t12 & 60 & FV & 65.0 & 8125 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t13 & 20 & RL & 96.0 & 12456 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t14 & 20 & RL & 90.0 & 14115 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t15 & 20 & RL & 75.0 & 10125 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t16 & 120 & RL & 55.0 & 7892 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t17 & 60 & RL & 70.0 & 8400 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t18 & 20 & RL & 65.0 & 8767 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t19 & 80 & RL & 65.0 & 10482 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t20 & 60 & RL & 92.0 & 9920 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t21 & 50 & RL & 55.0 & 7642 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t22 & 20 & RL & 69.5 & 19900 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t23 & 90 & RL & 92.0 & 12108 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t24 & 20 & RL & 78.0 & 10264 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t25 & 20 & RL & 85.0 & 11049 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t26 & 190 & RL & 107.0 & 10615 & Pave & Grvl & IR1 & Bnk & $\\dots$ \\\\\n",
       "\t27 & 50 & RL & 64.0 & 13053 & Pave & Pave & Reg & Bnk & $\\dots$ \\\\\n",
       "\t28 & 20 & RL & 70.0 & 7931 & Pave & Grvl & Reg & Lvl & $\\dots$ \\\\\n",
       "\t29 & 20 & RL & 93.0 & 15306 & Pave & Grvl & IR1 & HLS & $\\dots$ \\\\\n",
       "\t30 & 20 & RL & 77.0 & 9320 & Pave & Grvl & IR1 & Lvl & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m1168×79 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m MSSubClass \u001b[0m\u001b[1m MSZoning \u001b[0m\u001b[1m LotFrontage \u001b[0m\u001b[1m LotArea \u001b[0m\u001b[1m Street  \u001b[0m\u001b[1m Alley   \u001b[0m\u001b[1m LotShape\u001b[0m ⋯\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Int64      \u001b[0m\u001b[90m String7  \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m String3 \u001b[0m ⋯\n",
       "──────┼─────────────────────────────────────────────────────────────────────────\n",
       "    1 │         70  RM               60.0     9600  Pave     Grvl     Reg      ⋯\n",
       "    2 │         75  RM               60.0     7200  Pave     Grvl     Reg\n",
       "    3 │        120  RL               69.5     5814  Pave     Grvl     IR1\n",
       "    4 │         90  RL               75.0     9825  Pave     Grvl     Reg\n",
       "    5 │         90  RL               69.5     7032  Pave     Grvl     IR1      ⋯\n",
       "    6 │         20  RL               69.5    11120  Pave     Grvl     IR1\n",
       "    7 │         60  RL               76.0     9120  Pave     Grvl     Reg\n",
       "    8 │         50  RL               66.0    21780  Pave     Grvl     Reg\n",
       "    9 │         60  RL               63.0     8199  Pave     Grvl     Reg      ⋯\n",
       "   10 │         90  RL               69.5    11500  Pave     Grvl     IR1\n",
       "   11 │         20  RL               95.0    13651  Pave     Grvl     IR1\n",
       "  ⋮   │     ⋮          ⋮           ⋮          ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 1159 │         60  RL               76.0    10142  Pave     Grvl     IR1\n",
       " 1160 │        160  RL               44.0     5306  Pave     Grvl     IR1      ⋯\n",
       " 1161 │         60  RL               69.5    10304  Pave     Grvl     IR1\n",
       " 1162 │         60  RL               86.0    10562  Pave     Grvl     Reg\n",
       " 1163 │         90  RL               60.0     8544  Pave     Grvl     Reg\n",
       " 1164 │         60  RL               69.5     9900  Pave     Grvl     Reg      ⋯\n",
       " 1165 │        120  FV               47.0     4230  Pave     Pave     Reg\n",
       " 1166 │        160  RM               21.0     1680  Pave     Grvl     Reg\n",
       " 1167 │         20  RL               78.0     7800  Pave     Grvl     Reg\n",
       " 1168 │         20  RL               64.0     7314  Pave     Grvl     Reg      ⋯\n",
       "\u001b[36m                                                72 columns and 1147 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the id and target columns in a different variable\n",
    "ids = df[!, Symbol(id_feature)]\n",
    "target = df[!, Symbol(target_feature)]\n",
    "\n",
    "# Dropping the id and target from the DataFrame\n",
    "select!(df, Not([Symbol(id_feature), Symbol(target_feature)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding Categorical features\n",
    "<p>\n",
    "The id column is just an identifier for the training example, so we will exclude it during the encoding phase.<br>\n",
    "Target feature will be label encoded in the next step.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top 10 categories\n",
    "function get_top_categories(df, features, n=10)\n",
    "    top_cats = Dict()\n",
    "    for feature in features\n",
    "        col_data = df[!, feature]\n",
    "        category_counts = countmap(col_data)\n",
    "        sorted_categories = sort(collect(category_counts), by=x->x[2], rev=true)\n",
    "        \n",
    "        # Take minimum between n and the number of unique categories\n",
    "        num_categories = min(n, length(sorted_categories))\n",
    "        \n",
    "        top_cats[feature] = [x[1] for x in sorted_categories[1:num_categories]]\n",
    "    end\n",
    "    return top_cats\n",
    "end\n",
    "\n",
    "# Get top 10 categories for specific features\n",
    "top_categories = get_top_categories(df, categorical_features)\n",
    "\n",
    "# Function to one-hot encode only the top 10 categories\n",
    "function one_hot_top_categories!(df, top_categories)\n",
    "    for (feature, top_cats) in top_categories\n",
    "        if length(top_cats) == 2  # Handle the binary case\n",
    "            # Assuming the first category in top_cats is treated as 'true'\n",
    "            new_col_name = \"$(feature)_binary\"\n",
    "            df[!, new_col_name] = df[!, feature] .== top_cats[1]\n",
    "        else  # Handle the general case\n",
    "            for cat in top_cats\n",
    "                new_col_name = \"$(feature)_$(cat)\"\n",
    "                df[!, new_col_name] = df[!, feature] .== cat\n",
    "            end\n",
    "        end\n",
    "        select!(df, Not(Symbol(feature)))  # Drop the original feature column\n",
    "    end\n",
    "end\n",
    "\n",
    "# Apply one-hot encoding\n",
    "one_hot_top_categories!(df, top_categories)\n",
    "\n",
    "# Serialize the imputation_values dictionary to a binary file\n",
    "open(TOP_CATEGORIES, \"w\") do io\n",
    "    serialize(io, top_categories)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Regressor\n",
    "We choose Linear Regression model, but feel free to try your own and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate the new DataFrame with the existing 'df' DataFrame horizontally\n",
    "df = hcat(df, DataFrame(target = target))\n",
    "\n",
    "# Get all column names\n",
    "all_columns = names(df)\n",
    "\n",
    "# Remove the target variable to get only predictor variables\n",
    "predictor_columns = filter(x -> x != \"target\", all_columns)\n",
    "\n",
    "# Create Terms for predictors and response\n",
    "predictor_terms = [Term(Symbol(col)) for col in predictor_columns]\n",
    "response_term = Term(:target)\n",
    "\n",
    "# Create the formula\n",
    "formula_obj = FormulaTerm(response_term, sum(predictor_terms))\n",
    "\n",
    "# Create a Linear Regression model and train it\n",
    "model = lm(formula_obj, df)\n",
    "\n",
    "# Your model is now trained on all predictor variables in 'df'\n",
    "\n",
    "# BEGIN\n",
    "    # If you want to perform additional operations on the model, you can do so here\n",
    "    # For example, you can set hyperparameters, do cross-validation, etc.\n",
    "    # model = ...\n",
    "# END\n",
    "\n",
    "# Saving the model to use it for predictions\n",
    "open(PREDICTOR_FILE_PATH, \"w\") do io\n",
    "    serialize(io, model)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
